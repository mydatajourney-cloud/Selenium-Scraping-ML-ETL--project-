# spark_transform âš¡ğŸ

## ğŸ“Œ Giá»›i thiá»‡u
`spark_transform` lÃ  má»™t dá»± Ã¡n minh hoáº¡ **ETL pipeline vá»›i Apache Spark**.  
Má»¥c tiÃªu lÃ  **load dá»¯ liá»‡u thÃ´**, **transform báº±ng Spark**, vÃ  **xuáº¥t káº¿t quáº£ cuá»‘i cÃ¹ng** á»Ÿ Ä‘á»‹nh dáº¡ng Ä‘Ã£ chuáº©n hoÃ¡, cÃ³ thá»ƒ dÃ¹ng cho phÃ¢n tÃ­ch hoáº·c Ä‘Æ°a vÃ o há»‡ thá»‘ng downstream.

---

## ğŸ“‚ Cáº¥u trÃºc dá»± Ã¡n
```
spark_transform/
â”œâ”€â”€ Dockerfile            # Docker hoÃ¡ mÃ´i trÆ°á»ng Spark + Python
â”œâ”€â”€ requirements.txt      # CÃ¡c thÆ° viá»‡n Python cáº§n thiáº¿t
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py        # BÆ°á»›c Extract: táº£i / Ä‘á»c dá»¯ liá»‡u tá»« nguá»“n (CSV, API, DB)
â”‚   â”œâ”€â”€ transform.py      # BÆ°á»›c Transform: lÃ m sáº¡ch, chuáº©n hoÃ¡, join, enrich dá»¯ liá»‡u
â”‚   â”œâ”€â”€ load.py           # BÆ°á»›c Load: ghi dá»¯ liá»‡u káº¿t quáº£ ra file (parquet, csv) hoáº·c DB
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/            # Dá»¯ liá»‡u gá»‘c (raw data)
â”‚   â””â”€â”€ output/           # Dá»¯ liá»‡u sau khi transform
â”‚
â”œâ”€â”€ README.md             # TÃ i liá»‡u giá»›i thiá»‡u dá»± Ã¡n
```

---

## ğŸ³ Dockerfile
`Dockerfile` giÃºp khá»Ÿi cháº¡y mÃ´i trÆ°á»ng Ä‘á»“ng nháº¥t, gá»“m:
- CÃ i Ä‘áº·t **Apache Spark + Hadoop** (phiÃªn báº£n lightweight cho local cluster)
- Python runtime + thÆ° viá»‡n cáº§n thiáº¿t (`pyspark`, `pandas`, v.v.)
- Thiáº¿t láº­p working directory `/app`

CÃ¡ch build & run:
```bash
docker build -t spark_transform .
docker run -it --rm -v $(pwd):/app spark_transform
```

---

## âš™ï¸ CÃ¡c file chÃ­nh

### 1. `extract.py`  
- Äá»c dá»¯ liá»‡u Ä‘áº§u vÃ o tá»« `./data/input/` (vÃ­ dá»¥: CSV, JSON, API)  
- Chuáº©n hoÃ¡ Ä‘á»‹nh dáº¡ng thÃ nh **Spark DataFrame**  

### 2. `transform.py`  
- Thá»±c hiá»‡n cÃ¡c bÆ°á»›c transform:  
  - Loáº¡i bá» giÃ¡ trá»‹ null / trÃ¹ng láº·p  
  - Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u  
  - Join dá»¯ liá»‡u tá»« nhiá»u báº£ng  
  - Táº¡o thÃªm cá»™t tÃ­nh toÃ¡n má»›i (vÃ­ dá»¥: tá»•ng, trung bÃ¬nh, phÃ¢n loáº¡i)  

### 3. `load.py`  
- LÆ°u dá»¯ liá»‡u transform ra thÆ° má»¥c `./data/output/`  
- Há»— trá»£ nhiá»u Ä‘á»‹nh dáº¡ng: **Parquet, CSV, JSON**  
- CÃ³ thá»ƒ má»Ÿ rá»™ng Ä‘á»ƒ load vÃ o DB (Postgres, BigQuery, v.v.)  

---

## ğŸ¯ Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c
- Dá»¯ liá»‡u thÃ´ ban Ä‘áº§u Ä‘Æ°á»£c **chuáº©n hoÃ¡ vÃ  lÃ m sáº¡ch** báº±ng Spark  
- Káº¿t quáº£ Ä‘áº§u ra náº±m trong thÆ° má»¥c:
```
data/output/
```
VÃ­ dá»¥:
```bash
data/output/cleaned_data.parquet
data/output/summary.csv
```

> Sau khi transform, dá»¯ liá»‡u cÃ³ thá»ƒ dÃ¹ng ngay cho **phÃ¢n tÃ­ch BI**, **ML training pipeline**, hoáº·c **náº¡p vÃ o data warehouse**.

---

## â–¶ï¸ CÃ¡ch cháº¡y pipeline

1. **Cháº¡y Extract**  
```bash
python src/extract.py
```

2. **Cháº¡y Transform**  
```bash
python src/transform.py
```

3. **Cháº¡y Load**  
```bash
python src/load.py
```

> CÃ³ thá»ƒ káº¿t há»£p thÃ nh pipeline ETL hoÃ n chá»‰nh, vÃ­ dá»¥ trong `Makefile` hoáº·c Airflow DAG.

---

## ğŸ“ Ghi chÃº
- ThÃ­ch há»£p cho xá»­ lÃ½ dá»¯ liá»‡u vá»«a & lá»›n báº±ng Spark  
- CÃ³ thá»ƒ má»Ÿ rá»™ng Ä‘á»ƒ cháº¡y trÃªn cluster tháº­t (YARN, Kubernetes, EMR) thay vÃ¬ local  
- Tá»‘i Æ°u khi dá»¯ liá»‡u input á»Ÿ dáº¡ng **Parquet/ORC**  
